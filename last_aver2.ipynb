{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4d88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 21:02:34 - INFO - 3328762466 - PyTorch version: 2.4.0\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - --- Starting Connectivity Tensor Preprocessing for Deep Learning ---\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Output directory: preprocessed_connectomes_for_dl\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Loading global tensor from: /home/diego/Escritorio/desde_cero/AAL_116/AAL_116_fmri_tensor_1lag_NeuroEnhanced_v4/GLOBAL_TENSOR_AAL116_352subs_4ch_1lag.npz\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Global tensor loaded. Shape: (352, 4, 116, 116), Subjects in tensor: 352\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Loading metadata from: /home/diego/Escritorio/desde_cero/AAL_116/DataBaseSubjects.csv\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Metadata loaded. Shape: (352, 24), Unique subjects in metadata: 352\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Aligning tensor data with metadata...\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Data alignment complete. Final aligned subjects: 352\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Encoding diagnostic labels...\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Label encoding complete. Value counts:\n",
      "ResearchGroup\n",
      "1    168\n",
      "2     95\n",
      "0     89\n",
      "Name: count, dtype: int64\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Performing Stratified 5-Fold Cross-Validation. Random state: 42\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - --- Processing Fold 1/5 ---\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Train set: 281 samples, Val set: 71 samples\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Train labels distribution: [ 71 134  76]\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Val labels distribution: [18 34 19]\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Fold 1 Train Age Stats: Mean=74.28, Std=7.95\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Fold 1 Val Age Stats:   Mean=73.26, Std=7.45\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Fold 1 Train Sex Dist: {'M': 151, 'F': 130}\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Fold 1 Val Sex Dist:   {'M': 36, 'F': 35}\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Normalizing tensors per fold (Z-score per channel, stats from training set)...\n",
      "2025-05-25 21:02:34 - INFO - 3328762466 - Tensor normalization complete for this fold.\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Saved preprocessed data for fold 1 to: preprocessed_connectomes_for_dl/fold_0/fold_0_preprocessed_data.pt\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - --- Processing Fold 2/5 ---\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Train set: 281 samples, Val set: 71 samples\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Train labels distribution: [ 71 134  76]\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Val labels distribution: [18 34 19]\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 2 Train Age Stats: Mean=74.25, Std=7.96\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 2 Val Age Stats:   Mean=73.36, Std=7.42\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 2 Train Sex Dist: {'M': 144, 'F': 137}\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 2 Val Sex Dist:   {'M': 43, 'F': 28}\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Normalizing tensors per fold (Z-score per channel, stats from training set)...\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Tensor normalization complete for this fold.\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Saved preprocessed data for fold 2 to: preprocessed_connectomes_for_dl/fold_1/fold_1_preprocessed_data.pt\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - --- Processing Fold 3/5 ---\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Train set: 282 samples, Val set: 70 samples\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Train labels distribution: [ 71 135  76]\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Val labels distribution: [18 33 19]\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 3 Train Age Stats: Mean=73.93, Std=7.64\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 3 Val Age Stats:   Mean=74.66, Std=8.68\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 3 Train Sex Dist: {'M': 144, 'F': 138}\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Fold 3 Val Sex Dist:   {'M': 43, 'F': 27}\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Normalizing tensors per fold (Z-score per channel, stats from training set)...\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Tensor normalization complete for this fold.\n",
      "2025-05-25 21:02:35 - INFO - 3328762466 - Saved preprocessed data for fold 3 to: preprocessed_connectomes_for_dl/fold_2/fold_2_preprocessed_data.pt\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - --- Processing Fold 4/5 ---\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Train set: 282 samples, Val set: 70 samples\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Train labels distribution: [ 71 135  76]\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Val labels distribution: [18 33 19]\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 4 Train Age Stats: Mean=73.93, Std=7.87\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 4 Val Age Stats:   Mean=74.65, Std=7.82\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 4 Train Sex Dist: {'M': 157, 'F': 125}\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 4 Val Sex Dist:   {'F': 40, 'M': 30}\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Normalizing tensors per fold (Z-score per channel, stats from training set)...\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Tensor normalization complete for this fold.\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Saved preprocessed data for fold 4 to: preprocessed_connectomes_for_dl/fold_3/fold_3_preprocessed_data.pt\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - --- Processing Fold 5/5 ---\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Train set: 282 samples, Val set: 70 samples\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Train labels distribution: [ 72 134  76]\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Val labels distribution: [17 34 19]\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 5 Train Age Stats: Mean=73.98, Std=7.89\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 5 Val Age Stats:   Mean=74.45, Std=7.77\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 5 Train Sex Dist: {'M': 152, 'F': 130}\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Fold 5 Val Sex Dist:   {'M': 35, 'F': 35}\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Normalizing tensors per fold (Z-score per channel, stats from training set)...\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Tensor normalization complete for this fold.\n",
      "2025-05-25 21:02:36 - INFO - 3328762466 - Saved preprocessed data for fold 5 to: preprocessed_connectomes_for_dl/fold_4/fold_4_preprocessed_data.pt\n",
      "2025-05-25 21:02:37 - INFO - 3328762466 - --- All folds processed and data saved. ---\n",
      "2025-05-25 21:02:37 - INFO - 3328762466 - Preprocessed data for each fold saved in subdirectories under: preprocessed_connectomes_for_dl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "\n",
    "# --- 0. Global Configuration and Constants ---\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Script Configuration ---\n",
    "# Paths\n",
    "BASE_OUTPUT_DIR = Path('./preprocessed_connectomes_for_dl') # Main directory to save preprocessed data\n",
    "TENSOR_DATA_PATH = Path('/home/diego/Escritorio/desde_cero/AAL_116/AAL_116_fmri_tensor_1lag_NeuroEnhanced_v4/GLOBAL_TENSOR_AAL116_352subs_4ch_1lag.npz') # Path to the global .npz tensor file\n",
    "METADATA_CSV_PATH = Path('/home/diego/Escritorio/desde_cero/AAL_116/DataBaseSubjects.csv') # Path to the CSV with subject metadata\n",
    "\n",
    "# Preprocessing Parameters\n",
    "N_FOLDS = 5  # Number of folds for Stratified K-Fold Cross-Validation\n",
    "RANDOM_STATE = 42  # Seed for reproducibility in shuffling and splitting\n",
    "AGE_COLUMN = 'Age' # Column name for age in metadata CSV\n",
    "SEX_COLUMN = 'Sex' # Column name for sex in metadata CSV (e.g., 'Male', 'Female')\n",
    "DIAGNOSIS_COLUMN = 'ResearchGroup' # Column name for diagnosis in metadata CSV\n",
    "SUBJECT_ID_COLUMN = 'SubjectID' # Column name for subject ID in metadata CSV\n",
    "\n",
    "# Label Mapping and Grouping for MCI\n",
    "# As per plan: CN:0, MCI_grouped:1, AD:2\n",
    "LABEL_MAPPING = {\n",
    "    'CN': 0,\n",
    "    'MCI': 1, 'LMCI': 1, 'EMCI': 1, # Grouping all MCI variants\n",
    "    'AD': 2\n",
    "}\n",
    "TARGET_CLASSES = ['CN', 'MCI_grouped', 'AD'] # For reference\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_and_align_data(tensor_path: Path, metadata_path: Path, subject_id_col: str) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads the global connectivity tensor and subject metadata, aligning them by subject ID.\n",
    "\n",
    "    Args:\n",
    "        tensor_path (Path): Path to the .npz file containing the global tensor and subject IDs.\n",
    "        metadata_path (Path): Path to the CSV file with subject metadata.\n",
    "        subject_id_col (str): Column name for subject IDs in the metadata CSV.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[np.ndarray], Optional[pd.DataFrame], Optional[np.ndarray]]:\n",
    "            - Aligned global tensor (N_subjects, N_channels, N_ROIs, N_ROIs).\n",
    "            - Aligned metadata DataFrame.\n",
    "            - Array of aligned subject IDs.\n",
    "        Returns (None, None, None) if loading or alignment fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading global tensor from: {tensor_path}\")\n",
    "    if not tensor_path.exists():\n",
    "        logger.error(f\"Tensor file not found: {tensor_path}\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        tensor_data_npz = np.load(tensor_path)\n",
    "        global_tensor = tensor_data_npz['global_tensor_data'] # Key used during saving in previous script\n",
    "        tensor_subject_ids = tensor_data_npz['subject_ids']\n",
    "        # channel_names = tensor_data_npz['channel_names'] # Available if needed later\n",
    "        logger.info(f\"Global tensor loaded. Shape: {global_tensor.shape}, Subjects in tensor: {len(tensor_subject_ids)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading tensor data from {tensor_path}: {e}\", exc_info=True)\n",
    "        return None, None, None\n",
    "\n",
    "    logger.info(f\"Loading metadata from: {metadata_path}\")\n",
    "    if not metadata_path.exists():\n",
    "        logger.error(f\"Metadata file not found: {metadata_path}\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        metadata_df[subject_id_col] = metadata_df[subject_id_col].astype(str).str.strip()\n",
    "        logger.info(f\"Metadata loaded. Shape: {metadata_df.shape}, Unique subjects in metadata: {metadata_df[subject_id_col].nunique()}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading metadata from {metadata_path}: {e}\", exc_info=True)\n",
    "        return None, None, None\n",
    "\n",
    "    # Align metadata with tensor data based on subject IDs\n",
    "    logger.info(\"Aligning tensor data with metadata...\")\n",
    "    tensor_sids_df = pd.DataFrame({subject_id_col: tensor_subject_ids})\n",
    "    # Ensure tensor_subject_ids are strings for merging, similar to metadata_df\n",
    "    tensor_sids_df[subject_id_col] = tensor_sids_df[subject_id_col].astype(str).str.strip()\n",
    "\n",
    "    # Merge to get metadata for subjects present in the tensor, in the tensor's order\n",
    "    aligned_metadata_df = pd.merge(tensor_sids_df, metadata_df, on=subject_id_col, how='left')\n",
    "\n",
    "    if len(aligned_metadata_df) != len(tensor_subject_ids):\n",
    "        logger.warning(f\"Mismatch in length after merging: {len(aligned_metadata_df)} vs {len(tensor_subject_ids)}. Some tensor subjects might not be in metadata.\")\n",
    "    \n",
    "    # Check for subjects in tensor but not in metadata (NaNs in merged columns other than SubjectID)\n",
    "    missing_metadata_count = aligned_metadata_df.drop(columns=[subject_id_col]).isnull().all(axis=1).sum()\n",
    "    if missing_metadata_count > 0:\n",
    "        logger.warning(f\"{missing_metadata_count} subjects from the tensor file do not have corresponding entries in the metadata file.\")\n",
    "        # Option: Filter out subjects with missing metadata if critical columns are NaN\n",
    "        # For now, we proceed but this should be handled based on requirements.\n",
    "\n",
    "    # Final check: ensure the order of aligned_metadata_df matches tensor_subject_ids\n",
    "    # The merge with tensor_sids_df (which is ordered like the tensor) should preserve this.\n",
    "    if not all(aligned_metadata_df[subject_id_col].values == tensor_subject_ids):\n",
    "        logger.error(\"CRITICAL: Subject ID order mismatch between tensor and aligned metadata after merge. This should not happen with a left merge on tensor IDs.\")\n",
    "        # This would be a critical issue, re-evaluate merging strategy if it occurs.\n",
    "        # For safety, re-index metadata to match tensor order if necessary, though left merge should handle it.\n",
    "        # temp_aligned_df = aligned_metadata_df.set_index(subject_id_col).reindex(tensor_subject_ids).reset_index()\n",
    "        # if not temp_aligned_df[subject_id_col].equals(pd.Series(tensor_subject_ids)):\n",
    "        #     logger.error(\"Re-indexing failed to align subject IDs.\")\n",
    "        #     return None, None, None\n",
    "        # aligned_metadata_df = temp_aligned_df\n",
    "\n",
    "    logger.info(f\"Data alignment complete. Final aligned subjects: {len(aligned_metadata_df)}\")\n",
    "    return global_tensor, aligned_metadata_df, tensor_subject_ids\n",
    "\n",
    "\n",
    "def encode_labels(metadata_df: pd.DataFrame, diagnosis_col: str, label_mapping: Dict[str, int]) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Encodes diagnostic labels based on the provided mapping, grouping MCI subtypes.\n",
    "\n",
    "    Args:\n",
    "        metadata_df (pd.DataFrame): DataFrame containing subject metadata.\n",
    "        diagnosis_col (str): Name of the column with diagnostic labels.\n",
    "        label_mapping (Dict[str, int]): Dictionary to map string labels to integers.\n",
    "\n",
    "    Returns:\n",
    "        Optional[np.ndarray]: NumPy array of encoded integer labels. Returns None if encoding fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"Encoding diagnostic labels...\")\n",
    "    if diagnosis_col not in metadata_df.columns:\n",
    "        logger.error(f\"Diagnosis column '{diagnosis_col}' not found in metadata.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the diagnosis column is string type before mapping\n",
    "    metadata_df[diagnosis_col] = metadata_df[diagnosis_col].astype(str).str.strip()\n",
    "    \n",
    "    # Apply the mapping\n",
    "    encoded_labels = metadata_df[diagnosis_col].map(label_mapping)\n",
    "\n",
    "    # Check for unmapped labels (NaNs after mapping)\n",
    "    if encoded_labels.isnull().any():\n",
    "        unmapped_values = metadata_df[encoded_labels.isnull()][diagnosis_col].unique()\n",
    "        logger.warning(f\"Unmapped diagnosis values found: {unmapped_values}. These subjects will have NaN labels.\")\n",
    "        # Decide how to handle: drop these subjects, assign a default label, or raise error.\n",
    "        # For now, they will remain NaN and might be dropped later if using StratifiedKFold.\n",
    "        # It's often better to ensure all labels are covered by the mapping or explicitly handled.\n",
    "        # For StratifiedKFold, NaNs in labels will cause errors.\n",
    "        # Let's filter out subjects with unmappable labels for robust splitting.\n",
    "        valid_indices = ~encoded_labels.isnull()\n",
    "        if not valid_indices.all():\n",
    "            logger.warning(f\"Dropping {np.sum(~valid_indices)} subjects due to unmappable diagnostic labels.\")\n",
    "            # This means the global_tensor and metadata_df need to be filtered accordingly *before* splitting.\n",
    "            # This filtering should ideally happen in the main function after load_and_align_data.\n",
    "            # For now, this function will return labels with NaNs, and the main logic must handle it.\n",
    "\n",
    "\n",
    "    logger.info(f\"Label encoding complete. Value counts:\\n{encoded_labels.value_counts(dropna=False)}\")\n",
    "    return encoded_labels.to_numpy()\n",
    "\n",
    "\n",
    "def normalize_connectivity_tensors_per_fold(\n",
    "    X_train: np.ndarray, X_val: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Normalizes connectivity tensors (Z-score) per channel.\n",
    "    Statistics (mean, std) are computed ONLY from X_train and applied to both X_train and X_val.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data tensors (N_train, N_channels, N_ROIs, N_ROIs).\n",
    "        X_val (np.ndarray): Validation data tensors (N_val, N_channels, N_ROIs, N_ROIs).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Normalized X_train_norm, X_val_norm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Normalizing tensors per fold (Z-score per channel, stats from training set)...\")\n",
    "    N_channels = X_train.shape[1]\n",
    "    X_train_norm = np.zeros_like(X_train, dtype=np.float32)\n",
    "    X_val_norm = np.zeros_like(X_val, dtype=np.float32)\n",
    "    epsilon = 1e-8 # To prevent division by zero if std is too small\n",
    "\n",
    "    for ch_idx in range(N_channels):\n",
    "        logger.debug(f\"Normalizing channel {ch_idx}...\")\n",
    "        # Calculate mean and std from the training data for the current channel\n",
    "        train_channel_data = X_train[:, ch_idx, :, :]\n",
    "        mean_ch = np.mean(train_channel_data)\n",
    "        std_ch = np.std(train_channel_data)\n",
    "        logger.debug(f\"Channel {ch_idx} - Train Mean: {mean_ch:.4f}, Train Std: {std_ch:.4f}\")\n",
    "\n",
    "        if std_ch < epsilon:\n",
    "            logger.warning(f\"Channel {ch_idx} has std deviation close to zero ({std_ch:.2e}) in training data. Normalization might be unstable or result in NaNs/Infs. Using epsilon for division.\")\n",
    "            std_ch_safe = epsilon # Avoid division by zero\n",
    "        else:\n",
    "            std_ch_safe = std_ch\n",
    "\n",
    "        # Normalize training data for the current channel\n",
    "        X_train_norm[:, ch_idx, :, :] = (train_channel_data - mean_ch) / std_ch_safe\n",
    "        \n",
    "        # Normalize validation data for the current channel using training statistics\n",
    "        val_channel_data = X_val[:, ch_idx, :, :]\n",
    "        X_val_norm[:, ch_idx, :, :] = (val_channel_data - mean_ch) / std_ch_safe\n",
    "        \n",
    "        # Verify normalization on training data (should be approx mean=0, std=1)\n",
    "        logger.debug(f\"Channel {ch_idx} - Normalized Train Mean: {np.mean(X_train_norm[:, ch_idx, :, :]):.4f}, Std: {np.std(X_train_norm[:, ch_idx, :, :]):.4f}\")\n",
    "        # Validation data will not necessarily have mean=0, std=1 after this process\n",
    "        logger.debug(f\"Channel {ch_idx} - Normalized Val Mean: {np.mean(X_val_norm[:, ch_idx, :, :]):.4f}, Std: {np.std(X_val_norm[:, ch_idx, :, :]):.4f}\")\n",
    "\n",
    "\n",
    "    logger.info(\"Tensor normalization complete for this fold.\")\n",
    "    return X_train_norm, X_val_norm\n",
    "\n",
    "# --- Main Preprocessing Script ---\n",
    "def main_preprocess():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire preprocessing pipeline:\n",
    "    1. Load data and metadata.\n",
    "    2. Encode labels.\n",
    "    3. Perform stratified K-fold splitting.\n",
    "    4. For each fold:\n",
    "        a. Normalize tensors.\n",
    "        b. Save preprocessed data for PyTorch.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Connectivity Tensor Preprocessing for Deep Learning ---\")\n",
    "\n",
    "    # Create base output directory if it doesn't exist\n",
    "    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Output directory: {BASE_OUTPUT_DIR}\")\n",
    "\n",
    "    # 1. Load and Align Data\n",
    "    global_tensor, metadata_df, aligned_subject_ids = load_and_align_data(TENSOR_DATA_PATH, METADATA_CSV_PATH, SUBJECT_ID_COLUMN)\n",
    "    if global_tensor is None or metadata_df is None:\n",
    "        logger.critical(\"Failed to load or align data. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2. Encode Labels\n",
    "    raw_labels = encode_labels(metadata_df, DIAGNOSIS_COLUMN, LABEL_MAPPING)\n",
    "    if raw_labels is None:\n",
    "        logger.critical(\"Failed to encode labels. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Filter out subjects with NaN labels (unmapped diagnoses) from tensor and metadata\n",
    "    valid_label_indices = ~np.isnan(raw_labels)\n",
    "    if not np.all(valid_label_indices):\n",
    "        num_dropped = np.sum(~valid_label_indices)\n",
    "        logger.warning(f\"Dropping {num_dropped} subjects due to missing/unmappable labels before splitting.\")\n",
    "        global_tensor = global_tensor[valid_label_indices]\n",
    "        metadata_df = metadata_df[valid_label_indices].reset_index(drop=True)\n",
    "        encoded_labels = raw_labels[valid_label_indices].astype(int) # Now safe to convert to int\n",
    "        final_subject_ids = aligned_subject_ids[valid_label_indices]\n",
    "        logger.info(f\"Data filtered. New tensor shape: {global_tensor.shape}, New metadata shape: {metadata_df.shape}\")\n",
    "    else:\n",
    "        encoded_labels = raw_labels.astype(int)\n",
    "        final_subject_ids = aligned_subject_ids\n",
    "\n",
    "    if len(np.unique(encoded_labels)) < 2:\n",
    "        logger.critical(f\"Not enough classes after label encoding ({len(np.unique(encoded_labels))} found). Need at least 2 for stratified splitting. Aborting.\")\n",
    "        return\n",
    "    \n",
    "    # Extract Age and Sex for potential secondary stratification or analysis\n",
    "    # Ensure these columns exist and handle missing values if necessary\n",
    "    ages = metadata_df[AGE_COLUMN].values if AGE_COLUMN in metadata_df else None\n",
    "    sexes = metadata_df[SEX_COLUMN].values if SEX_COLUMN in metadata_df else None\n",
    "    \n",
    "    if ages is None: logger.warning(f\"Age column '{AGE_COLUMN}' not found in metadata. Age-based stratification/analysis will not be possible.\")\n",
    "    if sexes is None: logger.warning(f\"Sex column '{SEX_COLUMN}' not found in metadata. Sex-based stratification/analysis will not be possible.\")\n",
    "\n",
    "\n",
    "    # 3. Stratified K-Fold Splitting\n",
    "    logger.info(f\"Performing Stratified {N_FOLDS}-Fold Cross-Validation. Random state: {RANDOM_STATE}\")\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # The `split` method yields (train_indices, val_indices)\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(global_tensor, encoded_labels)):\n",
    "        logger.info(f\"--- Processing Fold {fold_idx + 1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = global_tensor[train_indices], global_tensor[val_indices]\n",
    "        y_train, y_val = encoded_labels[train_indices], encoded_labels[val_indices]\n",
    "        \n",
    "        train_sids = final_subject_ids[train_indices]\n",
    "        val_sids = final_subject_ids[val_indices]\n",
    "\n",
    "        logger.info(f\"Train set: {len(X_train)} samples, Val set: {len(X_val)} samples\")\n",
    "        logger.info(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
    "        logger.info(f\"Val labels distribution: {np.bincount(y_val)}\")\n",
    "\n",
    "        # Log distribution of Age and Sex in this fold (if available)\n",
    "        if ages is not None:\n",
    "            logger.info(f\"Fold {fold_idx+1} Train Age Stats: Mean={np.mean(ages[train_indices]):.2f}, Std={np.std(ages[train_indices]):.2f}\")\n",
    "            logger.info(f\"Fold {fold_idx+1} Val Age Stats:   Mean={np.mean(ages[val_indices]):.2f}, Std={np.std(ages[val_indices]):.2f}\")\n",
    "        if sexes is not None:\n",
    "            train_sex_counts = pd.Series(sexes[train_indices]).value_counts().to_dict()\n",
    "            val_sex_counts = pd.Series(sexes[val_indices]).value_counts().to_dict()\n",
    "            logger.info(f\"Fold {fold_idx+1} Train Sex Dist: {train_sex_counts}\")\n",
    "            logger.info(f\"Fold {fold_idx+1} Val Sex Dist:   {val_sex_counts}\")\n",
    "        # NOTE: For more rigorous multi-label stratification (diagnosis, age, sex),\n",
    "        # one might need iterative stratification techniques or create composite labels.\n",
    "        # This script primarily stratifies by diagnosis. The logs above help verify secondary balance.\n",
    "\n",
    "        # 4. Normalization (per fold, stats from train set)\n",
    "        X_train_norm, X_val_norm = normalize_connectivity_tensors_per_fold(X_train, X_val)\n",
    "\n",
    "        # Convert to PyTorch Tensors\n",
    "        X_train_tensor = torch.from_numpy(X_train_norm).float()\n",
    "        y_train_tensor = torch.from_numpy(y_train).long() # long for CrossEntropyLoss\n",
    "        X_val_tensor = torch.from_numpy(X_val_norm).float()\n",
    "        y_val_tensor = torch.from_numpy(y_val).long()\n",
    "\n",
    "        # 5. Save Preprocessed Data for PyTorch\n",
    "        fold_data = {\n",
    "            'X_train': X_train_tensor,\n",
    "            'y_train': y_train_tensor,\n",
    "            'train_subject_ids': train_sids,\n",
    "            'X_val': X_val_tensor,\n",
    "            'y_val': y_val_tensor,\n",
    "            'val_subject_ids': val_sids\n",
    "        }\n",
    "        \n",
    "        fold_output_dir = BASE_OUTPUT_DIR / f\"fold_{fold_idx}\"\n",
    "        fold_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        output_file_path = fold_output_dir / f\"fold_{fold_idx}_preprocessed_data.pt\"\n",
    "        try:\n",
    "            torch.save(fold_data, output_file_path)\n",
    "            logger.info(f\"Saved preprocessed data for fold {fold_idx + 1} to: {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data for fold {fold_idx + 1}: {e}\", exc_info=True)\n",
    "\n",
    "        # Clean up to save memory before next fold\n",
    "        del X_train, X_val, y_train, y_val, X_train_norm, X_val_norm\n",
    "        del X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, fold_data\n",
    "        gc.collect()\n",
    "\n",
    "    logger.info(\"--- All folds processed and data saved. ---\")\n",
    "    logger.info(f\"Preprocessed data for each fold saved in subdirectories under: {BASE_OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure PyTorch is available\n",
    "    try:\n",
    "        import torch\n",
    "        logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    except ImportError:\n",
    "        logger.critical(\"PyTorch is not installed. Please install PyTorch to run this script.\")\n",
    "        exit()\n",
    "        \n",
    "    main_preprocess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
